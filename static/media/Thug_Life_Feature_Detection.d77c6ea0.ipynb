{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ10-HSW7Inm"
   },
   "source": [
    "# **Feature detection and overlaying**\n",
    "\n",
    "In this section we will create overlaying function and our main function for feature retrieval and then overlaying the props over those identified features.\n",
    "\n",
    "Download the rar containing props from [here](https://drive.google.com/file/d/1-fPGb4NBhEBN9hbC-rYMlea8vta6Dlc7/view?usp=sharing) and extract in your working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdOEsNfHo8pu"
   },
   "source": [
    "**Haar Cascade Classifiers**\n",
    "\n",
    "OpenCV already contains many pre-trained classifiers for face, eyes, smile etc. XML files containing the wights of the model are stored in opencv/data/haarcascades/ folder and can be used for feature detection quickly and easily.\n",
    "\n",
    "OpenCV's algorithm is uses the following Haar-like features which are the input to the basic classifiers:\n",
    "\n",
    "<img src=\"https://www.bogotobogo.com/python/OpenCV_Python/images/FaceDetection/HaarFeatures.png\" align=\"center\"/>\n",
    "\n",
    "In short here is how feature detection works, instead of applying all the 6000 features on a window, group the features into different stages of classifiers and apply one-by-one. \n",
    "\n",
    "If a window fails the first stage, discard it. We don't consider remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region.\n",
    "\n",
    "For example:\n",
    "\n",
    "<img src=\"https://www.bogotobogo.com/python/OpenCV_Python/images/FaceDetection/stages.png\" align=\"center\"/>\n",
    "\n",
    "Complete discussion about the Haar Cascade Classifiers is beyond the scope of our current discussion but checkout *Further Readings* section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIm9woOu4lXu"
   },
   "source": [
    "**Alpha Blending for overlaying**\n",
    "\n",
    "Alpha blending is the process of overlaying a foreground image with transparency over a background Image. The transparent image is generally a PNG image which consists of four channels (RGBA). The fourth channel is the alpha channel which holds the transparency magnitude.\n",
    "Here background image will be our source image and foreground image will be thug life props.\n",
    "\n",
    "At every pixel of the image, we blend the background and foreground image color(F) and background color (B) using the alpha mask.\n",
    "\n",
    "Explained clearly [here](https://pytech-solution.blogspot.com/2017/07/alphablending.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHERpsbD4yLJ"
   },
   "outputs": [],
   "source": [
    "def transparentOverlay(src, overlay, pos=(0, 0), scale=1):\n",
    "    '''\n",
    "    This function is used to blend the props over the identified features\n",
    "    Takes four arguments.\n",
    "    src : this is the original image (frames â€“ as we loop over the frames from the video stream)\n",
    "    overlay : this refers to the overlay images (shades and cigar)\n",
    "    position and scaling factor\n",
    "    '''\n",
    "\n",
    "    # For changing the dimension of the image\n",
    "    overlay = cv2.resize(overlay, (0, 0), fx=scale, fy=scale)\n",
    "    \n",
    "    # Size of foreground\n",
    "    h, w, _ = overlay.shape\n",
    "    # Size of background Image\n",
    "    rows, cols, _ = src.shape\n",
    "\n",
    "    y, x = pos[0], pos[1]  # Position of foreground/overlay image\n",
    " \n",
    "    # Loop over all pixels and apply the blending equation\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # Check the bounding conditions\n",
    "            if x + i >= rows or y + j >= cols:\n",
    "                continue\n",
    "            # Read the alpha channel\n",
    "            alpha = float(overlay[i][j][3] / 255.0)\n",
    "            src[x + i][y + j] = alpha * overlay[i][j][:3] + (1 - alpha) * src[x + i][y + j]\n",
    "    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I4-Bb1uX6N0"
   },
   "outputs": [],
   "source": [
    "def ThugLife(src, specs, cigar):\n",
    "    '''\n",
    "    A void function that takes 3 arguments and transforms the input image to image with thug life filter applied\n",
    "    src: string that conatins path to image file\n",
    "    specs: path to png specs prop as in thug life playground\n",
    "    cigar: path to png cigar prop as in thug life playground\n",
    "    '''\n",
    "\n",
    "    # Reading the images\n",
    "    img = cv2.imread(src, -1)\n",
    "    specs = cv2.imread(specs, -1)\n",
    "    cigar = cv2.imread(cigar,-1)\n",
    "\n",
    "    cv2.imshow('Original',img)\n",
    "\n",
    "    # Converting coloured images to black and white and hence reducing the computation by a factor of 3 [Coloured images: 3 channels, Gray images: 1 channel]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Loads the weights of cascade classifier for faces from the file (You don't need to download the xml files ! They are present in the library)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Used to find faces from the image. If eyes are found, it returns the positions of detected faces as Rect(x,y,w,h)\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 5, 0, (120, 120), (350, 350))\n",
    "\n",
    "    #  Iterating through the faces and creating Region of Interest (RoI) and then overlaying\n",
    "    for (x, y, w, h) in faces:\n",
    "        if h > 0 and w > 0:\n",
    "          # Calculating the region where specs are to be overlayed\n",
    "          glass_ymin = int(y + 1.5 * h / 5)\n",
    "          glass_ymax = int(y + 2.5 * h / 5)\n",
    "          h_glass = glass_ymax - glass_ymin\n",
    "\n",
    "          # Calculating the region where cigar is to be overlayed\n",
    "          cigar_ymin = int(y + 4 * h / 6)\n",
    "          cigar_ymax = int(y + 5.5 * h / 6)\n",
    "          h_cigar = cigar_ymax - cigar_ymin\n",
    "\n",
    "          # Note that we are going to overlay over the color image but the RoI was created over Gray image\n",
    "          glass_roi = img[glass_ymin:glass_ymax, x:x+w]\n",
    "          cigar_roi = img[cigar_ymin:cigar_ymax, x:x+w]\n",
    "\n",
    "          # Resizing the props according to INTER_CUBIC interpolation (Best for resizing, but slow)\n",
    "          specs = cv2.resize(specs, (w, h_glass),interpolation=cv2.INTER_CUBIC)\n",
    "          cigar= cv2.resize(cigar, (w, h_cigar),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "          # Applying the overlays, calling the helper function\n",
    "          transparentOverlay(glass_roi,specs)\n",
    "          transparentOverlay(cigar_roi,cigar,(int(w/2),int(h_cigar/2)))\n",
    "\n",
    "    cv2.imshow('ThugLife', img)\n",
    "\n",
    "    # Writing the file and storing in current directory\n",
    "    cv2.imwrite('output.jpg', img)\n",
    "\n",
    "    # Image will be displayed infinitely until any keypress\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyVYpwVxouxe"
   },
   "source": [
    "# **Testing the functions**\n",
    "\n",
    "In this section we will first load the props already provided with the folder and provide the path for source image and check the functions we created.\n",
    "\n",
    "Two windows will open displaying the original image and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW0SOJfPigLF"
   },
   "outputs": [],
   "source": [
    "# Path to props\n",
    "specs = '.img/specs.png'\n",
    "cigar = '.img/cigar.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAKNBIDumEqM"
   },
   "outputs": [],
   "source": [
    "# Add path to input image\n",
    "img = \"image.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEw3O_czojBP"
   },
   "outputs": [],
   "source": [
    "ThugLife(img, specs, cigar)\n",
    "# Or you can also do this, Now whenever space will be clicked your image will be captured and then transformed\n",
    "ThugLife(CaptureImage(), specs, cigar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSisO-saJgvK"
   },
   "source": [
    "**Expected Output**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/viveksb007/ThugLifeImgCreator/master/sshots/s3.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZiFDWqd1LTt"
   },
   "source": [
    "# **Further Reading**\n",
    "\n",
    "Congrats !! Now you have the recipe for transforming the images as done in the Thug Life playground of our website.\n",
    "\n",
    "Also read and continue building more cool stuff -\n",
    "\n",
    "*  [Rapid Object Detection using a Boosted Cascade of Simple Features](https://www.researchgate.net/publication/3940582_Rapid_Object_Detection_using_a_Boosted_Cascade_of_Simple_Features)\n",
    "\n",
    "* [Image filtering and processing using Adversarial Networks](http://www.jetir.org/papers/JETIR1703019.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Thug_Life_Feature_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
